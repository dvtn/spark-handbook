Mind Map generated by NB MindMap plugin   
> __version__=`1.1`,showJumps=`true`
---

# SparkCore

## Spark
> collapsed=`true`


### Spark是分布式计算框架

### Spark底层操作的是RDD

### 与MR的区别
> collapsed=`true`


#### 1\.Spark基于内存迭代处理数据，MR是基于磁盘迭代处理数据

#### 2\.Spark中有DAG有向无环图执行引擎，执行速度快

#### 3\.Spark粗粒度资源申请，MR是细粒度资源申请

#### 4\.MR中只有mapper和reducer，相当于spark中的map和reduceByKey两个算子，在MR业务逻辑要自己实现，spark中有各种算子对应各种业务

## Spark技术栈
> collapsed=`true`


### HDFS,MR,Yarn

### Hive, Storm

### SparkCore, SparkStreaming, SparkSQL, SparkMlLib

## Spark运行模式
> collapsed=`true`


### local
> collapsed=`true`


#### 用于本地IDEA,eclipse开发，多用于测试

### standalone
> collapsed=`true`


#### Spark自带的资源调度框架

#### 支持分布式搭建

#### Spark可以基于standalone集群提交任务

### yarn
> collapsed=`true`


#### Hadoop生态圈中的资源调度框架

#### Spark也支持在yarn中运行

### mesos
> collapsed=`true`


#### 资源调度框架

## Spark代码流程
> collapsed=`true`


### 1\.创建 val conf = new SparkConf\(\)\.setMaster\("\.\.\."\)\.setAppName\("\.\.\."\)

### 2\.创建 val sc = new SparkContext\(conf\)

### 3\.得到RDD: val rdd = sc\.textFile\("\.\.\."\)

### 4\.对得到的RDD使用Transformation类算子进行数据转换

### 5\.使用Action类算子触发执行

### 6\.sc\.stop\(\)

## Spark核心RDD
> collapsed=`true`


### RDD\(Resilient Distributed Dataset\):弹性分布式数据集

### 五大特性
> collapsed=`true`


#### 1\.RDD由一系列partition组成

#### 2\.算子是作用在partition上的;

#### 3\.RDD之间有依赖关系

#### 4\.分区器是作用在K,V格式的RDD上

#### 5\.partition对外提供最佳的计算位置，利于数据处理的本地化

### 问题
> collapsed=`true`


#### 1\.什么是KV格式的RDD?
> collapsed=`true`


##### RDD中的元素是一个个的tuple2

#### 2\.sc\.textFile\(\.\.\.\)
> collapsed=`true`


##### 底层调用的是MR读取HDFS的方法, 首先也会split,一个split对应一个block, 这里的split也对应一个partition

#### 3\.哪里体现了RDD的分布式？
> collapsed=`true`


##### RDD中的partition是分布在多个节点上的

#### 4\.哪里体现了RDD的弹性？
> collapsed=`true`


##### 1\.partition的个数可多可少

##### 2\.RDD之间有依赖关系

### RDD中是不存数据的，partition中也不存数据

## Spark算子
> collapsed=`true`


### Transformation\(懒执行，需要Action类算子触发\)
> collapsed=`true`


#### filter

#### map

#### flatMap

#### reduceByKey

#### sortBy

#### sortByKey

#### sample

#### join

#### leftOuterJoin

#### rightOuterJoin

#### fullOuterJoin

#### union

#### intersection

#### subtract

#### distinct
> collapsed=`true`


##### map\+reduceByKey\+map

#### cogroup

#### mapPartitions

#### mapPartitionWithIndex

#### coalesce\(int numPartitions, boolean shuffle=false\)

#### repartition
> collapsed=`true`


##### 是有shuffle类的算子

##### repartition=coalesce\(int numPartitions, true\)

#### groupByKey
> collapsed=`true`


##### KV格式

#### zip
> collapsed=`true`


##### 压缩成KV格式

#### zipWithIndex
> collapsed=`true`


##### 给RDD中的元素与当前元素的下标压缩成KV格式

### Action\(触发Transformation类算子执行,一个Application中有几个Action算子，就有几个job\)
> collapsed=`true`


#### count

#### foreach

#### collect

#### first
> collapsed=`true`


##### first=take\(1\)

#### take

#### foreachPartition

#### reduce

#### countByKey

#### countByValue

### 持久化算子
> collapsed=`true`


#### cache
> collapsed=`true`


##### 默认将数据存在内存中

##### cache\(\)=persist\(\)=perist\(StorageLevel\.MEMORY\_ONLY\)

#### persist
> collapsed=`true`


##### 可以动指定数据的持久化级别

##### StorageLevel
> collapsed=`true`


###### MEMORY\_ONLY

###### MEMORY\_ONLY\_SER

###### MEMORY\_AND\_DISK

###### MEMORY\_AND\_DISK\_SER

#### checkPoint
> collapsed=`true`


##### 可以将数据持久化到磁盘

##### 切断RDD间的依赖关系

##### 当lineage非常长，计算又复杂时，可以使用checkPoint对RDD进行持久化

##### 当application执行完毕后，checkPoint中的数据不会被清除

##### checkPoint执行流程
> collapsed=`true`


###### 1\.当application有action算子触发执行时，job执行完成后，会从后往前回溯

###### 2\.回溯去找哪些RDD被checkpoint做标记

###### 3\.回溯完成后，重新计算checkpoint RDD的数据，将结果写入指定的checkpoint目录中

###### 4\.切断RDD的依赖关系

###### 优化：对RDD进行checkpoint之前，最好先cache一下

#### 注意
> collapsed=`true`


##### 1\.cache和persist的最小单位是partition, 懒执行,需要action算子触发

##### 2\.对一个RDD使用cache或者persist后，可以赋值给一个变量，下次直接使用这个变量就是使用持久化的数据

##### 3\.当Application执行完毕之后，cache和persist持久化的数据会被清除

## Spark集群搭建
> collapsed=`true`


### 1\.上传解压包

### 2\.配置 \.\./conf/slaves
> collapsed=`true`


#### 配置worker节点

### 3\.配置 \.\./conf/spark\-env\.sh
> collapsed=`true`


#### SPARK\_MASTER\_IP=node1
> collapsed=`true`


##### Master节点

#### SPARK\_MASTER\_PORT=7077
> collapsed=`true`


##### 提交任务的端口

#### SPARK\_WORKER\_CORES=2
> collapsed=`true`


##### 配置worker使用的cores

#### SPARK\_WORKER\_MEMORY=3g
> collapsed=`true`


##### 配置worker节点支配内存

### 4\.将配置好的安装包发送到其它节点
> collapsed=`true`


#### scp \-r \.\./spark\.\.\.node2:\`pwd\`

### 5\.在Master节点启动集群
> collapsed=`true`


#### \.\./sbin/start\-all\.sh

### 6\.WEBUI查看集群
> collapsed=`true`


#### http://node1:8080

#### 修改端口
> collapsed=`true`


##### 1\.配置\.\./conf/spark\-env\.sh
> collapsed=`true`


###### SPARK\_MASTER\_WEBUI\_PORT=9999

##### 2\.修改Master节点中的\.\./sbin/start\-master\.sh

##### 3\.临时导入环境变量
> collapsed=`true`


###### export SPARK\_MASTER\_WEBUI\_PORT=8888

###### 删除临时环境变量
> collapsed=`true`


####### export \-n SPARK\_MASTER\_WEBUI\_PORT=8888

## Spark客户端
> collapsed=`true`


### 原封不动将安装包拷贝到一台新的节点

## SparkPi任务提交
> collapsed=`true`


### \.\./sbin/spark\-submit \-\-master spark://node1:7077 \-\-class org\.apache\.spark\.examples\.SparkPi \.\./lib/spark\-examples\.\.\.\.jar 参数

## Spark基于yarn提交任务配置
> collapsed=`true`


### 在客户端\.\./conf/spark\-env\.sh中配置
> collapsed=`true`


#### HADOOP\_CONF\_DIR=$HADOOP\_HOME/etc/hadoop

## Spark任务提交
> collapsed=`true`


### standalone
> collapsed=`true`


#### client
> collapsed=`true`


##### 命令
> collapsed=`true`


###### \./spark\-submit \-\-master spark://node1:7077 \-\-class \.\.\.jar \.\.\.参数 

###### \./spark\-submit \-\-master spark://node1:7077 \-\-deploy\-mode client \-\-class \.\.\.jar\.\.\.参数

##### 1\.在客户端提交application，Driver在客户端启动

##### 2\.客户端向Master申请资源,Master返回worker节点

##### 3\.Driver向Worker节点发送task,监控task执行，回收结果

##### 总结：client模式提交任务，适用于程序测试，不适用于生产。Driver会在客户端启动，当在客户端提交多个application时，会有网卡流量激增问题。在客户端可以看到task执行和结果

#### cluster
> collapsed=`true`


##### 命令
> collapsed=`true`


###### \./spark\-submit \-\-master spark://node1:7077 \-\-deploy\-mode cluster \-\-class \.\.\.jar\.\.\.参数

##### 1\.在客户端提交Application,首先客户端向Master申请启动Driver

##### 2\.Master随机在一台worker中启动Driver

##### 3\.Driver启动后，Driver向Master申请资源，Master返回资源

##### 4\.Driver发送task, 监控task, 回收结果

##### 总结：cluster模式提交任务，适用于生产环境。Driver是在集群中的某一台worker节点启动，会将客户端网卡流量激增问题分散到集群

#### Driver的功能
> collapsed=`true`


##### 1\.发送task

##### 2\.监控task

##### 3\.申请资源

##### 4\.回收结果

### yarn
> collapsed=`true`


#### client
> collapsed=`true`


##### 命令
> collapsed=`true`


###### \./spark\-submit \-\-master yarn \-\-class\.\.\.jar\.\.\.

###### \./spark\-submit \-\-master yarn\-client \-\-class \.\.\.jar \.\.\.

###### \./spark\-submit \-\-master yarn \-\-deploy\-mode client \-\-class \.\.\.jar\.\.\. 

##### 1\.在客户端提交Application,Driver在客户端启动

##### 2\.客户端向ResourceManager申请启动ApplicationMaster

##### 3\.RM收到请求后，随机在一台NodeManager节点上启动ApplicationMaster

##### 4\.ApplicationMaster启动之后，向ResourceManager申请资源，用于启动Executor

##### 5\.ResourceManager收到请求后，返回给ApplicationMaster一批NodeManager节点

##### 6\.ApplicationMaster连接NodeManager启动Executor

##### 7\.Executor启动之后，反向注册给Driver

##### 8\.Driver发送task, 监控task, 回收结果

##### 总结：client模式提交任务，适用于程序测试，不适用于生产。Driver会在客户端启动，当在客户端提交多个application时，会有网卡流量激增问题。在客户端可以看到task执行和结果

#### cluster
> collapsed=`true`


##### 命令
> collapsed=`true`


###### \./spark\-submit \-\-master yarn\-cluster \-\-class \.\.\.jar\.\.\.

###### \./spark\-submit \-\-master yarn \-\-deploy\-mode cluster \-\-class \.\.\.jar\.\.\.

##### 1\.在客户端提交Application,首先客户端向ResourceManager申请启动ApplicationMaster

##### 2\.RM收到请求后，随机在一台NodeManager节点上启动ApplicationMaster\(Driver\)

##### 3\.ApplicationMaster启动之后，向ResourceManager申请资源，用于启动Executor

##### 4\.ResourceManager收到请求后，返回给ApplicationMaster一批NodeManager节点

##### 5\.ApplicationMaster连接NodeManager启动Executor

##### 6\.Executor启动之后，反向注册给ApplicationMaster\(Driver\)

##### 7\.ApplicationMaster\(Driver\)发送task, 监控task, 回收结果

##### 总结：cluster模式提交任务，适用于生产环境。Driver是在集群中的某一台NodeManager节点启动，会将客户端网卡流量激增问题分散到集群,在客户端看不到task执行和结果，要去webui中查看

#### ApplicationMaster作用
> collapsed=`true`


##### 1\.申请资源

##### 2\.启动Executor

##### 3\.任务调度

## 术语
> collapsed=`true`


### 任务层面
> collapsed=`true`


#### application\-\>job\-\>stage\-\>task

### 资源层面
> collapsed=`true`


#### master\-\>worker\-\>executor\-\>threadpool

## RDD宽窄依赖
> collapsed=`true`


### 窄依赖
> collapsed=`true`


#### 父RDD与子RDD parition之间的关系是一对一

#### 父RDD与子RDD partition之间的关系是多对一

### 宽依赖\(shuffle\)
> collapsed=`true`


#### 父RDD与子RDD partition之间的关系是一对多

## Stage
> collapsed=`true`


### 由一组并行的task组成

### Stage计算模式
> collapsed=`true`


#### pipeline管道计算模式

#### partition上是计算逻辑

#### 一条条处理数据

### Stage的并行度
> collapsed=`true`


#### 由stage中的final RDD的partition个数决定

### 如何提高stage并行度?
> collapsed=`true`


#### reduceByKey\(xx, numpartitions\)

#### join\(xxx,numpartitions\)

#### \.\.\.

### 管道中的数据何时落地？
> collapsed=`true`


#### shuffle write

#### 对RDD持久化

## Spark的资源调度和任务调度
> collapsed=`true`


### 资源调度
> collapsed=`true`


#### 1\.启动集群，worker向master汇报资源，master掌握了集群资源

#### 2\.当new SparkContext时，会创建两个对象:DAGScheduler和TaskScheduler

#### 3\.TaskScheduler向master申请资源

#### 4\.master收到请求，找到满足资源的worker,启动executor

#### 5\.executor启动后反向注册给TaskScheduler, Driver掌握了一批计算资源

### 任务调度
> collapsed=`true`


#### 6\.application中有一个action算子，就有一个job, job中有RDD, RDD会形成一个DAG有向无环图

#### 7\.DAG负责将每个job中的DAG按照RDD的宽窄依赖切割job, 划分stage, 将stage以TaskSet提交给TaskScheduler

#### 8\.TaskScheduler会遍历TaskSet, 拿到一个个task, 将task发送到Executor中的ThreadPool执行

#### 9\.TaskScheduler监控task执行，回收结果

### 总结
> collapsed=`true`


#### 1\.TaskScheduler不仅可以重试发送失败的task, 重试3次，如果这个task依然失败，由DAGScheduler负责重试，重试4次，如果依然失败，那么job失败，application就失败

#### 2\.TaskScheduler不仅可以重试失败的task，还可以重试执行缓慢的task,这是spark中的推测执行机制，对于ETL的业务场景要关闭

#### 3\.如果遇到application一直执行不完的情况
> collapsed=`true`


##### 1\.是否遇到了数据倾斜

##### 2\.是否开启了推测执行
> collapsed=`true`


###### spark\.speculation\(default:false\)

## 粗粒度资源申请和细粒度资源申请
> collapsed=`true`


### 粗粒度资源申请
> collapsed=`true`


#### 优点
> collapsed=`true`


##### application执行之前将所有的资源申请完毕，task执行时就不用自己申请资源，task执行快，application执行就快了

#### 缺点
> collapsed=`true`


##### 集群资源不能充分利用

### 细粒度资源申请
> collapsed=`true`


#### 优点
> collapsed=`true`


##### 集群资源能充分利用

#### 缺点
> collapsed=`true`


##### application执行之前不会将所有的资源申请完毕，task执行时自己申请资源，task执行完毕后立即释放资源

## 广播变量

### 当Executor端使用到了Driver端的变量

#### 1\.如果不使用广播变量，在一个Executor中有多少task,就有多少变量副本；

#### 2\.如果使用广播变量，在每个Executor中只有一份Driver端的变量副本

### 注意

#### 1\.不能将RDD广播出去,可以将RDD的结果广播出去

#### 2\.广播变量只能在Driver端定义，在Executor端使用，不能在Executor端改变

## 累加器

### 相当于集群中的统筹大变量

### 注意

#### 1\.累加器只能在Driver端定义初始化，不能在Executor端定义初始化

#### 2\.累加器取值accumulator\.value只能在Driver端执行,不能在Executor端\.value读值
