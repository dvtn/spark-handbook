Mind Map generated by NB MindMap plugin   
> __version__=`1.1`,showJumps=`true`
---

# SparkCore

## Spark
> collapsed=`true`


### Spark是分布式计算框架

### Spark底层操作的是RDD

### 与MR的区别
> collapsed=`true`


#### 1\.Spark基于内存迭代处理数据，MR是基于磁盘迭代处理数据

#### 2\.Spark中有DAG有向无环图执行引擎，执行速度快

#### 3\.Spark粗粒度资源申请，MR是细粒度资源申请

#### 4\.MR中只有mapper和reducer，相当于spark中的map和reduceByKey两个算子，在MR业务逻辑要自己实现，spark中有各种算子对应各种业务

## Spark技术栈
> collapsed=`true`


### HDFS,MR,Yarn

### Hive, Storm

### SparkCore, SparkStreaming, SparkSQL, SparkMlLib

## Spark运行模式
> collapsed=`true`


### local
> collapsed=`true`


#### 用于本地IDEA,eclipse开发，多用于测试

### standalone
> collapsed=`true`


#### Spark自带的资源调度框架

#### 支持分布式搭建

#### Spark可以基于standalone集群提交任务

### yarn
> collapsed=`true`


#### Hadoop生态圈中的资源调度框架

#### Spark也支持在yarn中运行

### mesos
> collapsed=`true`


#### 资源调度框架

## Spark代码流程
> collapsed=`true`


### 1\.创建 val conf = new SparkConf\(\)\.setMaster\("\.\.\."\)\.setAppName\("\.\.\."\)

### 2\.创建 val sc = new SparkContext\(conf\)

### 3\.得到RDD: val rdd = sc\.textFile\("\.\.\."\)

### 4\.对得到的RDD使用Transformation类算子进行数据转换

### 5\.使用Action类算子触发执行

### 6\.sc\.stop\(\)

## Spark核心RDD
> collapsed=`true`


### RDD\(Resilient Distributed Dataset\):弹性分布式数据集

### 五大特性
> collapsed=`true`


#### 1\.RDD由一系列partition组成

#### 2\.算子是作用在partition上的;

#### 3\.RDD之间有依赖关系

#### 4\.分区器是作用在K,V格式的RDD上

#### 5\.partition对外提供最佳的计算位置，利于数据处理的本地化

### 问题
> collapsed=`true`


#### 1\.什么是KV格式的RDD?
> collapsed=`true`


##### RDD中的元素是一个个的tuple2

#### 2\.sc\.textFile\(\.\.\.\)
> collapsed=`true`


##### 底层调用的是MR读取HDFS的方法, 首先也会split,一个split对应一个block, 这里的split也对应一个partition

#### 3\.哪里体现了RDD的分布式？
> collapsed=`true`


##### RDD中的partition是分布在多个节点上的

#### 4\.哪里体现了RDD的弹性？
> collapsed=`true`


##### 1\.partition的个数可多可少

##### 2\.RDD之间有依赖关系

### RDD中是不存数据的，partition中也不存数据

## Spark算子
> collapsed=`true`


### Transformation\(懒执行，需要Action类算子触发\)
> collapsed=`true`


#### filter

#### map

#### flatMap

#### reduceByKey

#### sortBy

#### sortByKey

#### sample

#### join

#### leftOuterJoin

#### rightOuterJoin

#### fullOuterJoin

#### union

#### intersection

#### subtract

#### distinct
> collapsed=`true`


##### map\+reduceByKey\+map

#### cogroup

#### mapPartitions

#### mapPartitionWithIndex

#### coalesce\(int numPartitions, boolean shuffle=false\)

#### repartition
> collapsed=`true`


##### 是有shuffle类的算子

##### repartition=coalesce\(int numPartitions, true\)

#### groupByKey
> collapsed=`true`


##### KV格式

#### zip
> collapsed=`true`


##### 压缩成KV格式

#### zipWithIndex
> collapsed=`true`


##### 给RDD中的元素与当前元素的下标压缩成KV格式

### Action\(触发Transformation类算子执行,一个Application中有几个Action算子，就有几个job\)
> collapsed=`true`


#### count

#### foreach

#### collect

#### first
> collapsed=`true`


##### first=take\(1\)

#### take

#### foreachPartition

#### reduce

#### countByKey

#### countByValue

### 持久化算子
> collapsed=`true`


#### cache
> collapsed=`true`


##### 默认将数据存在内存中

##### cache\(\)=persist\(\)=perist\(StorageLevel\.MEMORY\_ONLY\)

#### persist
> collapsed=`true`


##### 可以动指定数据的持久化级别

##### StorageLevel
> collapsed=`true`


###### MEMORY\_ONLY

###### MEMORY\_ONLY\_SER

###### MEMORY\_AND\_DISK

###### MEMORY\_AND\_DISK\_SER

#### checkPoint
> collapsed=`true`


##### 可以将数据持久化到磁盘

##### 切断RDD间的依赖关系

##### 当lineage非常长，计算又复杂时，可以使用checkPoint对RDD进行持久化

##### 当application执行完毕后，checkPoint中的数据不会被清除

##### checkPoint执行流程
> collapsed=`true`


###### 1\.当application有action算子触发执行时，job执行完成后，会从后往前回溯

###### 2\.回溯去找哪些RDD被checkpoint做标记

###### 3\.回溯完成后，重新计算checkpoint RDD的数据，将结果写入指定的checkpoint目录中

###### 4\.切断RDD的依赖关系

###### 优化：对RDD进行checkpoint之前，最好先cache一下

#### 注意
> collapsed=`true`


##### 1\.cache和persist的最小单位是partition, 懒执行,需要action算子触发

##### 2\.对一个RDD使用cache或者persist后，可以赋值给一个变量，下次直接使用这个变量就是使用持久化的数据

##### 3\.当Application执行完毕之后，cache和persist持久化的数据会被清除

## Spark集群搭建
> collapsed=`true`


### 1\.上传解压包

### 2\.配置 \.\./conf/slaves
> collapsed=`true`


#### 配置worker节点

### 3\.配置 \.\./conf/spark\-env\.sh
> collapsed=`true`


#### SPARK\_MASTER\_IP=node1
> collapsed=`true`


##### Master节点

#### SPARK\_MASTER\_PORT=7077
> collapsed=`true`


##### 提交任务的端口

#### SPARK\_WORKER\_CORES=2
> collapsed=`true`


##### 配置worker使用的cores

#### SPARK\_WORKER\_MEMORY=3g
> collapsed=`true`


##### 配置worker节点支配内存

### 4\.将配置好的安装包发送到其它节点
> collapsed=`true`


#### scp \-r \.\./spark\.\.\.node2:\`pwd\`

### 5\.在Master节点启动集群
> collapsed=`true`


#### \.\./sbin/start\-all\.sh

### 6\.WEBUI查看集群
> collapsed=`true`


#### http://node1:8080

#### 修改端口
> collapsed=`true`


##### 1\.配置\.\./conf/spark\-env\.sh
> collapsed=`true`


###### SPARK\_MASTER\_WEBUI\_PORT=9999

##### 2\.修改Master节点中的\.\./sbin/start\-master\.sh

##### 3\.临时导入环境变量
> collapsed=`true`


###### export SPARK\_MASTER\_WEBUI\_PORT=8888

###### 删除临时环境变量
> collapsed=`true`


####### export \-n SPARK\_MASTER\_WEBUI\_PORT=8888

## Spark客户端
> collapsed=`true`


### 原封不动将安装包拷贝到一台新的节点

## SparkPi任务提交
> collapsed=`true`


### \.\./sbin/spark\-submit \-\-master spark://node1:7077 \-\-class org\.apache\.spark\.examples\.SparkPi \.\./lib/spark\-examples\.\.\.\.jar 参数

## Spark基于yarn提交任务配置
> collapsed=`true`


### 在客户端\.\./conf/spark\-env\.sh中配置
> collapsed=`true`


#### HADOOP\_CONF\_DIR=$HADOOP\_HOME/etc/hadoop

## Spark任务提交
> collapsed=`true`


### standalone
> collapsed=`true`


#### client
> collapsed=`true`


##### 命令
> collapsed=`true`


###### \./spark\-submit \-\-master spark://node1:7077 \-\-class \.\.\.jar \.\.\.参数 

###### \./spark\-submit \-\-master spark://node1:7077 \-\-deploy\-mode client \-\-class \.\.\.jar\.\.\.参数

##### 1\.在客户端提交application，Driver在客户端启动

##### 2\.客户端向Master申请资源,Master返回worker节点

##### 3\.Driver向Worker节点发送task,监控task执行，回收结果

##### 总结：client模式提交任务，适用于程序测试，不适用于生产。Driver会在客户端启动，当在客户端提交多个application时，会有网卡流量激增问题。在客户端可以看到task执行和结果

#### cluster
> collapsed=`true`


##### 命令
> collapsed=`true`


###### \./spark\-submit \-\-master spark://node1:7077 \-\-deploy\-mode cluster \-\-class \.\.\.jar\.\.\.参数

##### 1\.在客户端提交Application,首先客户端向Master申请启动Driver

##### 2\.Master随机在一台worker中启动Driver

##### 3\.Driver启动后，Driver向Master申请资源，Master返回资源

##### 4\.Driver发送task, 监控task, 回收结果

##### 总结：cluster模式提交任务，适用于生产环境。Driver是在集群中的某一台worker节点启动，会将客户端网卡流量激增问题分散到集群

#### Driver的功能
> collapsed=`true`


##### 1\.发送task

##### 2\.监控task

##### 3\.申请资源

##### 4\.回收结果

### yarn
> collapsed=`true`


#### client
> collapsed=`true`


##### 命令
> collapsed=`true`


###### \./spark\-submit \-\-master yarn \-\-class\.\.\.jar\.\.\.

###### \./spark\-submit \-\-master yarn\-client \-\-class \.\.\.jar \.\.\.

###### \./spark\-submit \-\-master yarn \-\-deploy\-mode client \-\-class \.\.\.jar\.\.\. 

##### 1\.在客户端提交Application,Driver在客户端启动

##### 2\.客户端向ResourceManager申请启动ApplicationMaster

##### 3\.RM收到请求后，随机在一台NodeManager节点上启动ApplicationMaster

##### 4\.ApplicationMaster启动之后，向ResourceManager申请资源，用于启动Executor

##### 5\.ResourceManager收到请求后，返回给ApplicationMaster一批NodeManager节点

##### 6\.ApplicationMaster连接NodeManager启动Executor

##### 7\.Executor启动之后，反向注册给Driver

##### 8\.Driver发送task, 监控task, 回收结果

##### 总结：client模式提交任务，适用于程序测试，不适用于生产。Driver会在客户端启动，当在客户端提交多个application时，会有网卡流量激增问题。在客户端可以看到task执行和结果

#### cluster
> collapsed=`true`


##### 命令
> collapsed=`true`


###### \./spark\-submit \-\-master yarn\-cluster \-\-class \.\.\.jar\.\.\.

###### \./spark\-submit \-\-master yarn \-\-deploy\-mode cluster \-\-class \.\.\.jar\.\.\.

##### 1\.在客户端提交Application,首先客户端向ResourceManager申请启动ApplicationMaster

##### 2\.RM收到请求后，随机在一台NodeManager节点上启动ApplicationMaster\(Driver\)

##### 3\.ApplicationMaster启动之后，向ResourceManager申请资源，用于启动Executor

##### 4\.ResourceManager收到请求后，返回给ApplicationMaster一批NodeManager节点

##### 5\.ApplicationMaster连接NodeManager启动Executor

##### 6\.Executor启动之后，反向注册给ApplicationMaster\(Driver\)

##### 7\.ApplicationMaster\(Driver\)发送task, 监控task, 回收结果

##### 总结：cluster模式提交任务，适用于生产环境。Driver是在集群中的某一台NodeManager节点启动，会将客户端网卡流量激增问题分散到集群,在客户端看不到task执行和结果，要去webui中查看

#### ApplicationMaster作用
> collapsed=`true`


##### 1\.申请资源

##### 2\.启动Executor

##### 3\.任务调度

## 术语
> collapsed=`true`


### 任务层面
> collapsed=`true`


#### application\-\>job\-\>stage\-\>task

### 资源层面
> collapsed=`true`


#### master\-\>worker\-\>executor\-\>threadpool

## RDD宽窄依赖
> collapsed=`true`


### 窄依赖
> collapsed=`true`


#### 父RDD与子RDD parition之间的关系是一对一

#### 父RDD与子RDD partition之间的关系是多对一

### 宽依赖\(shuffle\)
> collapsed=`true`


#### 父RDD与子RDD partition之间的关系是一对多

## Stage
> collapsed=`true`


### 由一组并行的task组成

### Stage计算模式
> collapsed=`true`


#### pipeline管道计算模式

#### partition上是计算逻辑

#### 一条条处理数据

### Stage的并行度
> collapsed=`true`


#### 由stage中的final RDD的partition个数决定

### 如何提高stage并行度?
> collapsed=`true`


#### reduceByKey\(xx, numpartitions\)

#### join\(xxx,numpartitions\)

#### \.\.\.

### 管道中的数据何时落地？
> collapsed=`true`


#### shuffle write

#### 对RDD持久化

## Spark的资源调度和任务调度
> collapsed=`true`


### 资源调度
> collapsed=`true`


#### 1\.启动集群，worker向master汇报资源，master掌握了集群资源

#### 2\.当new SparkContext时，会创建两个对象:DAGScheduler和TaskScheduler

#### 3\.TaskScheduler向master申请资源

#### 4\.master收到请求，找到满足资源的worker,启动executor

#### 5\.executor启动后反向注册给TaskScheduler, Driver掌握了一批计算资源

### 任务调度
> collapsed=`true`


#### 6\.application中有一个action算子，就有一个job, job中有RDD, RDD会形成一个DAG有向无环图

#### 7\.DAG负责将每个job中的DAG按照RDD的宽窄依赖切割job, 划分stage, 将stage以TaskSet提交给TaskScheduler

#### 8\.TaskScheduler会遍历TaskSet, 拿到一个个task, 将task发送到Executor中的ThreadPool执行

#### 9\.TaskScheduler监控task执行，回收结果

### 总结
> collapsed=`true`


#### 1\.TaskScheduler不仅可以重试发送失败的task, 重试3次，如果这个task依然失败，由DAGScheduler负责重试，重试4次，如果依然失败，那么job失败，application就失败

#### 2\.TaskScheduler不仅可以重试失败的task，还可以重试执行缓慢的task,这是spark中的推测执行机制，对于ETL的业务场景要关闭

#### 3\.如果遇到application一直执行不完的情况
> collapsed=`true`


##### 1\.是否遇到了数据倾斜

##### 2\.是否开启了推测执行
> collapsed=`true`


###### spark\.speculation\(default:false\)

## 粗粒度资源申请和细粒度资源申请
> collapsed=`true`


### 粗粒度资源申请
> collapsed=`true`


#### 优点
> collapsed=`true`


##### application执行之前将所有的资源申请完毕，task执行时就不用自己申请资源，task执行快，application执行就快了

#### 缺点
> collapsed=`true`


##### 集群资源不能充分利用

### 细粒度资源申请
> collapsed=`true`


#### 优点
> collapsed=`true`


##### 集群资源能充分利用

#### 缺点
> collapsed=`true`


##### application执行之前不会将所有的资源申请完毕，task执行时自己申请资源，task执行完毕后立即释放资源

## 广播变量
> collapsed=`true`


### 当Executor端使用到了Driver端的变量
> collapsed=`true`


#### 1\.如果不使用广播变量，在一个Executor中有多少task,就有多少变量副本；

#### 2\.如果使用广播变量，在每个Executor中只有一份Driver端的变量副本

### 注意
> collapsed=`true`


#### 1\.不能将RDD广播出去,可以将RDD的结果广播出去

#### 2\.广播变量只能在Driver端定义，在Executor端使用，不能在Executor端改变

## 累加器
> collapsed=`true`


### 相当于集群中的统筹大变量

### 注意
> collapsed=`true`


#### 1\.累加器只能在Driver端定义初始化，不能在Executor端定义初始化

#### 2\.累加器取值accumulator\.value只能在Driver端执行,不能在Executor端\.value读值

## Spark WebUI
> collapsed=`true`


### Spark Shell
> collapsed=`true`


#### 交互式scala编程

#### 创建好了SparkContext, SQLContext

### WebUI
> collapsed=`true`


#### Jobs\-\>Stages\-\>StorageLevel\-\>Environment\-\>SQL\-\>Streaming

#### job\-\>stage\-\>task

### 配置日志管理
> collapsed=`true`


#### spark\.eventLog\.enabled = true

#### spark\.eventLog\.dir=xxx

## HistoryServer配置
> collapsed=`true`


### 1\.在客户端\.\./conf/spark\-defaults\.conf
> collapsed=`true`


#### spark\.eventLog\.enabled = true

#### spark\.eventLog\.dir=xxx

#### spark\.eventLog\.compress = true

#### spark\.history\.fs\.logDirectory = hdfs://node01:9000/spark/log/
> collapsed=`true`


##### 历史服务器日志恢复路径

### 2\.在客户端启动HistoryServer
> collapsed=`true`


#### \.\./sbin/start\-history\-server\.sh

### 3\.访问历史服务器WEBUI
> collapsed=`true`


#### http://node04:18080

## 常用端口
> collapsed=`true`


### 50070

### 9000

### 8020

### 60010

### 2181

### 8080

### 8088

### 8081

### 4040

### 18080

### 9083

### 3306

### 9092

## Master HA
> collapsed=`true`


### Master是jvm进程有可能挂掉，当Master挂掉以后，不能提交启动Driver, 因此要搭建Master HA

### 搭建
> collapsed=`true`


#### FileSystem

#### Zookeeper
> collapsed=`true`


##### 分布式协调服务

##### 保存元数据

##### 自动故障切换

### 使用zookeeper搭建
> collapsed=`true`


#### 1\.在alive Master中的\.\./conf/spark\-env\.sh中配置
> collapsed=`true`


##### export SPARK\_DAEMON\_JAVA\_OPTS="\-Dspark\.deploy\.recoveryMode=ZOOKEEPER \-Dspark\.deploy\.zookeeper\.url=node02:2181,node03:2181,node04:2181 \-Dspark\.deploy\.zookeeper\.dir=/SparkHA"

#### 2\.分发该文件到各集群节点
> collapsed=`true`


##### scp \.\./conf/spark\-env\.sh node02:\`pwd\`

##### scp \.\./conf/spark\-env\.sh node03:\`pwd\`

#### 3\.在standby Master中的\.\./conf/spark\-env\.sh中配置
> collapsed=`true`


##### SPARK\_MASTER\_IP=node02

#### 4\.启动zookeeper

#### 5\.在alive master中启动spark集群
> collapsed=`true`


##### \./sbin/start\-all\.sh

#### 6\.在standby master中启动
> collapsed=`true`


##### \./sbin/start\-master\.sh

#### 7\.测试
> collapsed=`true`


##### 主备切换过程中不影响在集群中已经运行的任务

##### 主备切换的过程中，影响向master申请资源

## Spark Shuffle
> collapsed=`true`


### HashShuffle
> collapsed=`true`


#### 普通机制
> collapsed=`true`


##### 产生磁盘小文件数量
> collapsed=`true`


###### M\(map task数量\)\*R\(reduce task数量\)

##### 过程
> collapsed=`true`


###### 1\.Map task处理数据之后，写到buffer缓存区，buffer大小是32K, 个数与reduce task一致

###### 2\.每个buffer缓存区满32K溢写磁盘，每个buffer最终对应一个磁盘小文件

###### 3\.reduce task拉取数据

##### 存在问题
> collapsed=`true`


###### 1\.产生磁盘小文件多

###### 2\.shuffle write对象多

###### 3\.shuffle read对象多

###### 4\.对象多内存不足就会gc, gc还不满足就人OOM

###### 5\.shuffle磁盘小文件多，会造成节点连接多，连接多容易不稳定

#### 合并机制
> collapsed=`true`


##### 产生磁盘小文件数量
> collapsed=`true`


###### C\(core\)\*R\(reduce task\)

##### 过程
> collapsed=`true`


###### 1\.Map task处理完数据后，写到buffer缓存区，buffer大小是32k, 个数与reduce task一致

###### 2\.Executor中每个core共用一份buffer缓存区

###### 3\.每个buffer缓存满32K溢写磁盘，每个buffer最终对应一个磁盘小文件

###### 4\.reduce task拉取数据

### SortShuffle
> collapsed=`true`


#### 普通机制
> collapsed=`true`


##### 产生磁盘小文件数量
> collapsed=`true`


###### 2\*M\(map task\)

##### 过程
> collapsed=`true`


###### 1\.map task处理完数据后，首先写入一个5M数据结构

###### 2\.SortShuffle有不定期估算机制，来估算内存结构大小，当估算超过真实大小，会申请内存：2\*估算\-当前

###### 3\.申请到内存继续写入数据结构，申请不到会溢写磁盘

###### 4\.溢写磁盘过程中有排序，每批次1万条溢写，最终对应两个磁盘文件，一个索引文件，一个数据文件

###### 5\.reduce task拉取数据，首先读取索引，再拉取数据

#### bypass机制
> collapsed=`true`


##### 产生磁盘小文件数量
> collapsed=`true`


###### 2\*M\(map task\)

##### 过程
> collapsed=`true`


###### 1\.map task处理完数据后，首先写入一个5M数据结构

###### 2\.SortShuffle有不定期估算机制，来估算内存结构大小，当估算超过真实大小，会申请内存：2\*估算\-当前

###### 3\.申请到内存继续写入数据结构，申请不到会溢写磁盘

###### 4\.溢写磁盘过程中没有排序，每批次1万条溢写，最终对应两个磁盘文件，一个索引文件，一个数据文件

###### 5\.reduce task拉取数据，首先读取索引，再拉取数据

## Shuffle文件寻址
> collapsed=`true`


### 对象
> collapsed=`true`


#### MapOutputTracker:管理磁盘小文件
> collapsed=`true`


##### MapOutputTrackerMaster\(Driver\)

##### MapOutputTrackerWorker\(Executor\)

#### BlockManager:块管理
> collapsed=`true`


##### BlockManagerMaster\(Driver\)
> collapsed=`true`


###### DiskStore:管理磁盘数据

###### MemoryStore：管理内存数据

###### ConnectionManager：连接其它BlockManager

###### BlockTransferService:负责拉取数据

##### BlockManagerWorker\(Executor\)
> collapsed=`true`


###### DiskStore:管理磁盘数据

###### MemoryStore：管理内存数据

###### ConnectionManager：连接其它BlockManager

###### BlockTransferService:负责拉取数据

### 过程
> collapsed=`true`


#### 1\.map task处理完数据之后，将数据结果和落地磁盘的位置封装到MapStatus对象中，通过Worker中的MapOutputTrackerWorker汇报给Driver中的MapOutputTrackerMaster, Driver掌握了磁盘小文件的位置

#### 2\.reduce task处理数据之前，先向Driver中的MapOutputTrackerMaster要磁盘小文件的位置信息，Driver返回

#### 3\.reduce端通BlockManager中的ConnectionManager连接数据所在节点的BlockManager

#### 4\.连接上后，通过BlockManager中的BlockTransferService默认启动5个子线程去拉取数据，默认一次拉取的数据量不能超过48M

#### 5\.拉取过来的数据放在Executor中的shuffle内存中\(spark\.shuffle\.memoryFraction 0\.2\)

## Spark内存管理
> collapsed=`true`


### 静态内存管理
> collapsed=`true`


#### 0\.2: task运行

#### 0\.2: spark\.shuffle\.memoryFraction
> collapsed=`true`


##### 0\.2: 预留

##### 0\.8：shuffle聚合内存

#### 0\.6: spark\.storage\.memoryFraction
> collapsed=`true`


##### 0\.1:预留

##### 0\.9
> collapsed=`true`


###### 0\.2:反序列化数据\(spark\.storage\.unrollFraction\)

###### 0\.8:RDD缓存和广播变量

### 统一内存管理
> collapsed=`true`


#### 300M

#### 总\-300M
> collapsed=`true`


##### 0\.25: task运行

##### 0\.75
> collapsed=`true`


###### 0\.5:shuffle聚合内存

###### 0\.5: RDD缓存和广播变量\(spark\.memory\.storageFraction\)

## Spark Shuffle调优
> collapsed=`true`


### spark\.shuffle\.file\.buffer 32K

### spark\.reducer\.maxSizeInFlight 48M

### spark\.shuffle\.io\.maxRetries 3

### spark\.shuffle\.io\.retryWait 5s

### spark\.shuffle\.memoryFraction 0\.2

### spark\.shuffle\.manager hash|sort

### spark\.shuffle\.sort\.bypassMergeThreshold 200 \-\-针对SortShuffle

### spark\.shuffle\.consolidateFile \-\-针对HashShuffle

### 参数设置
> collapsed=`true`


#### 代码中设置 conf\.set\(K,V\)
> collapsed=`true`


##### 级别最高

#### spark\-defauls\.conf中设置
> collapsed=`true`


##### 第三级别

#### 在提交任务时设置 \./spark\-submit \-\-conf xxx= xxx
> collapsed=`true`


##### 第二级别
